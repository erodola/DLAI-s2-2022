{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformers_2022.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4C5Ct9yoZKYa"
      },
      "source": [
        "# Deep Learning & Applied AI\n",
        "\n",
        "We recommend to go through the notebook using Google Colaboratory.\n",
        "\n",
        "# Tutorial 11: Transformers\n",
        "\n",
        "\n",
        "In this tutorial, we will cover:\n",
        "\n",
        "- Attention Mechanism, Transformers\n",
        "\n",
        "\n",
        "Our info:\n",
        "\n",
        "- dr. Luca Moschella (moschella@di.uniroma1.it)\n",
        "- dr. Donato Crisostomi (crisostomi@di.uniroma1.it)\n",
        "\n",
        "Course:\n",
        "\n",
        "- Website and notebooks will be available at [DLAI-s2-2022](https://github.com/erodola/DLAI-s2-2022)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m01o2KvPhreo"
      },
      "source": [
        "## These are the days of the Transformers\n",
        " \n",
        "Transformers are the last big advancement in deep learning architectures. They acquired popularity in NLP but now are ubiquitous in the deep learning landscape, with disruptive applications in time series forecasting, tasks with 3D data, and even in computer vision where the throne of CNNs seemed established: [recently](https://arxiv.org/abs/2010.11929) a Transformer pushed forward the state of the art in image classification.\n",
        " \n",
        "What is the secret of Transformers? \n",
        " \n",
        "They leverage all the power of the [bitter lesson](http://www.incompleteideas.net/IncIdeas/BitterLesson.html), today their performance cap is determined only by hardware. Differently from CNNs or recurrent neural networks, they scale very well to GPU clusters and suffer less of vanishing gradients. The biggest neural networks we have trained so far are Transformers and their performance continues to increase with more data and trainable parameters (see Figure 3.1 of the [GPT-3 paper](https://arxiv.org/abs/2005.14165)).\n",
        " \n",
        "> **GOOGLE QUESTION** How many learnable parameters has GPT-3? How many parameters has the last InceptionNet or state of the art LSTM for some NLP task?\n",
        " \n",
        "Such enormous Transformers solved convincingly intelligent tasks where all other architectures failed. Tasks that we considered still prerogative of humans, like few shot learning (Figure 3.14, 3.16 of the GPT-3 paper) or convincing visual original compositions, like in [Dall-E](https://openai.com/dall-e-2/) by Open-AI. Two years ago a machine imagining a \"*blue elephant riding a unicycle on the moon*\" [do not seemed](https://www.qualcomm.com/news/onq/2020/05/13/far-ai-can-see-what-we-still-need-build-human-level-intelligence) in the immediate future.\n",
        " \n",
        "Let's see how to build the basic Transformer block."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_uXwltjTS_e"
      },
      "source": [
        "### The Tranformer block\n",
        "A Transformer block operates a sequence-to-sequence transformation. The core of the transformer block is the self-attention operation, the only moment when the information of an element of the sequence mixes with the others. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKE5gZaMYWIY"
      },
      "source": [
        "### Self-attention operation\n",
        " \n",
        "Given some input vectors $x_1, \\dots, x_t$, the self-attention operation generates the output vectors $y_1, \\dots, y_t$ through a simple weighted average:\n",
        " \n",
        "$$y_i = \\sum_j w_{ij}x_j \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\text{with}\\; \\sum_j w_{ij} = 1$$\n",
        " \n",
        "Intuitively we want the weights $w_{ij}$ to module the *attention* we should put on the element $x_j$ when calculating $y_i$.\n",
        " \n",
        "If we do not have any idea on how to compare $x_1, \\dots, x_t$, the only way to go is to rely just on the data prior and directly learn the $w_{ij}$.\n",
        " \n",
        "> **QUESTION:** Which architecture can we recognize in this procedure?\n",
        " \n",
        "Things change if we can establish the similarity between two input elements $x_1, \\dots, x_t$ through a dot product, in that case we could define weights as:\n",
        " \n",
        "$$w_{ij}= x_i^\\top x_j$$\n",
        " \n",
        "in this way the attention we are putting on the element $x_j$ to compute $y_i$ is proportional to the similarity between $x_i$ and $x_j$.\n",
        " \n",
        "> *Why is it a good idea to choose where to pay attention based on this similarity?*\n",
        " \n",
        ">Let's try to build an intuition using this mind-bending game where we should pay attention to sequences of emojis:\n",
        ">\n",
        ">| emoji  sequence                                                  |\n",
        "|------------------------------------------------------------|\n",
        " | âš«â—»ï¸ðŸ”¶â—¼ï¸ |\n",
        " | ðŸ”´ðŸ”µðŸ”¶ðŸ”´              |\n",
        "| â—¼ï¸ðŸ”´ðŸ”¶âš«          |\n",
        "| â—»ï¸ðŸ”µðŸ”¶ ? | \n",
        ">\n",
        ">To guess the fourth symbol in the last row you observe the other examples. What are you paying attention to in these other examples? Probably you are looking at the things in common between the fourth symbol and the others, ending up figuring out that you should pay attention to the first symbol to determine the color, and to the second symbol to determine the shape, ignoring the third symbol.\n",
        ">\n",
        ">If the color and shape information are encoded in the dimensions of the feature vector $x$ representing these symbols, you see how the formulation $w_{ij}= x_i^\\top x_j$ does a good job in modeling your attentive behaviour.\n",
        " \n",
        "Notice that $x_i^\\top x_j \\in (-\\infty , +\\infty)$, so to respect our normalization constraint we should rescale our weights, for example using a softmax:\n",
        " \n",
        "$$w_{ij} = \\frac{e^{w'_{ij}}}{\\sum _j e^{w'_{ij}}} \\;\\;\\;\\;\\;\\;\\;\\;\\; \\text{with} \\; w'_{ij}= x_i^\\top x_j$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFQfqJXo61Tz"
      },
      "source": [
        "#### But where is everybody[?](https://en.wikipedia.org/wiki/Fermi_paradox)\n",
        "\n",
        "Where are the learning parameters? When we use the convolution operation in CNNs, the weights of the filters convoluting our images are learned. In graph neural networks we [introduced](https://colab.research.google.com/github/erodola/DLAI-s2-2022/blob/main/labs/10/Geometric_deep_learning.ipynb) learnable parameters $\\alpha$ through a single transformation $\\tau_\\alpha$ altering the laplacian eigenvalues $\\lambda_1, \\dots, \\lambda_n$ in the spectral convolution operation. How can we introduce learnable parameters in the self-attention operation?\n",
        "\n",
        "We start by noting that in the self-attention operation the input vector $x_i$ is playing three roles at the same time, in fact [a](https://www.ilpost.it/2019/08/24/pierfrancesco-favino-50-anni/)ll the roles! \n",
        "\n",
        "\n",
        "- In the **Key** ($k$) role $x_i$ is compared every time to all the other vectors $x_j$ to determine a weight needed to compute the output vector $y_j$.\n",
        "- In the **Query** ($q$) role $x_i$  is transposed and compared to every other vector $x_j$ to determine all the weights needed to compute its own output $y_i$.\n",
        "- In the **Value** ($v$) role $x_i$ is directly used in the weighted sum to determine every output once we have the weights.\n",
        "\n",
        "\n",
        "$$ y_i = \\sum_j w_{ij}v_j \\;\\;\\;\\;\\;\\;\\;\\;\\;\\; \\text{with} \\;\\; w_{ij} = \\frac{e^{w'_{ij}}}{\\sum _j e^{w'_{ij}}} \\;\\;\\; \\text{and} \\;\\; w'_{ij}= q_i^\\top k_j $$\n",
        "\n",
        "\n",
        "![image](https://drive.google.com/uc?export=view&id=1Y8q1YkCCztx70FfWLjH37RG0BBz289bF)\n",
        "\n",
        "We are glad to work with the same formidable actor, but different roles may require different makeup and costumes.\n",
        "\n",
        "A very basic idea is to apply a different linear transformation to each role:\n",
        "\n",
        "$$k_i=W_k x_i \\;\\;; \\;\\;q_i = W_q x_i \\;\\;; \\;\\; v_i= W_v x_i $$\n",
        "\n",
        "Guess what, we are going to learn these matrices $W_k, W_q, W_v$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vWSfF5QYwQr"
      },
      "source": [
        "#### Many heads are better than one\n",
        "\n",
        "Our self-attention operation looks more and more like a neural network module, we have our learning parameters and everything is differentiable.\n",
        "\n",
        "We add two final tricks to make the gradients work well and empower the expressiveness of our formidable module:\n",
        "\n",
        "- We want to avoid big weights $w'_{ij}$ that once softmaxed would cause a gradient close to zero and therefore a great slow down of the learning process. Since the scale of a dot product grows with the number of dimension of the input vectors $x_i = (x_{i1}, \\dots, x_{im})$, we scale down $w'_{ij}$ by a $\\sqrt{m}$ factor:\n",
        "$$w'_{ij} = \\frac{(W_qx_i)^\\top (W_k x_j)}{\\sqrt{m}}$$\n",
        "\n",
        "> **QUESTION:** Can you figure out why $\\sqrt{m}$ is the correct scaling factor?\n",
        "\n",
        "- We have introduced the learnable $m \\times m$ matrices $W_k, W_q$ and $W_v$. Until now for every *key* role we multiply the input vector always by the same $W_k$, but is there only a way to be a *key*? Can an actor play with the same makeup and costume all the movie long? \n",
        "\n",
        "    Why not introducing multiple learnable matrices $W_k^1, W_k^2, \\dots W_k^r; W_q^1, \\dots W_q^r; W_v^1, \\dots W_v^r$ and run many self-attention operation in parallel. Think about CNNs, we learn many filters to alter the input, not just one. When we learn $r$ different matrices for each role, we say that we are using $r$ *attention heads*.\n",
        "\n",
        "    With $r$ heads we produce $r$ different outputs $y_i^r$ for each input vector $x_i$. Usually we combine the outputs through simple concatenation, in this way we have $m$-dimensional vectors in input and $r \\cdot m$-dimensional vectors in output. To obtain newly $m$-dimensional vectors in output we simply apply a final linear transformation.\n",
        "\n",
        "> **Implementation note:** Instead of calculating $r$ different $m \\times m$ linear transformations for each key, query and value: \n",
        "$$k_i^1 = W_k^1 x_i,\\;  k_i^2 = W_k^2 x_i, \\;\\dots, \\;k_i^r = W_k^r x_i, \\; q_i^1 = W_q^1 x_i, \\; \\dots, q_i^r = W_q^r x_i, \\; v_i^1 = W_v^1 x_i, \\; \\dots, \\; v_i^r = W_v^r x_i $$\n",
        "We can be faster by stacking the $r$ matrices per role in a single $r \\cdot m \\times m$ linear transformation to apply to the input, obtaining directly the concatenated output.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjNHg9jtg5VI"
      },
      "source": [
        "### Implementing the complete self-attention module\n",
        "\n",
        "Let's implement all what we have introduced so far in a single delightful PyTorch module. \n",
        "\n",
        "*Code cells of these sections are adapted from the very nice [tutorial](http://peterbloem.nl/blog/transformers) of Peter Bloem.*\n",
        "\n",
        "> **EXERCISE:** Try to implement the forward pass of the self-attention block by yourself. It may be easier to start without considering the batch dimension.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kMyogHHpmaz"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, m, heads=8):\n",
        "        self.m, self.heads = m, heads\n",
        "\n",
        "        # We create the key, query and value matrices already stacked\n",
        "        self.tokeys    = nn.Linear(m, m * heads, bias=False)\n",
        "        self.toqueries = nn.Linear(m, m * heads, bias=False)\n",
        "        self.tovalues  = nn.Linear(m, m * heads, bias=False)\n",
        "\n",
        "        # The final linear transformation to finish newly with m-dimensional vectors\n",
        "        self.mergeheads = nn.Linear(heads * m, m)\n",
        "    \n",
        "    def forward(self, x):\n",
        "\n",
        "        pass  # âœï¸ your code here \n",
        "        \n",
        "        return y"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9Kjg3-Ipyps"
      },
      "source": [
        "Below you find a solution using einsum."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UBOItigi0oH"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, m, heads=8):\n",
        "        self.m, self.heads = m, heads\n",
        "        \n",
        "        # We create the key, query and value matrices already stacked\n",
        "        self.tokeys    = nn.Linear(m, m * heads, bias=False)\n",
        "        self.toqueries = nn.Linear(m, m * heads, bias=False)\n",
        "        self.tovalues  = nn.Linear(m, m * heads, bias=False)\n",
        "\n",
        "        # The final linear transformation to finish newly with m-dimensional vectors\n",
        "        self.mergeheads = nn.Linear(heads * m, m)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        b, t, m = x.size()  # batch dimension, sequence length, input vector dimension\n",
        "        r = self.heads\n",
        "\n",
        "        # First, we obtain keys, queries, and values\n",
        "        # we reshape to have a separated dimension for heads\n",
        "        keys    = self.tokeys(x).view(b, t, r, m)  \n",
        "        queries = self.toqueries(x).view(b, t, r, m)\n",
        "        values  = self.tovalues(x).view(b, t, r, m)\n",
        "\n",
        "        # The dot product to obtain the weights should collapse the m dimension\n",
        "        w_prime = torch.einsum('btrm,bfrm->brtf', queries, keys) / math.sqrt(m)  \n",
        "        w = F.softmax(w_prime, dim=-1)\n",
        "\n",
        "        # The weighted sum should collapse f-length sequences of m-vectors to single m-vectors (f=t) \n",
        "        y_conc = torch.einsum('brtf,bfrm->btrm', w, values)\n",
        "\n",
        "        # Finally we have to merge the outputs from each head, so we should collapse the r dimension (k=m)\n",
        "        y_conc = torch.einsum('btrm,krm->btk', y_conc, self.mergeheads.weight.view(m,r,m)) \n",
        "        y = y_conc + self.mergeheads.bias\n",
        "        return y\n",
        "    "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8MvTquAqANl"
      },
      "source": [
        "### The implementation of a Transformer block\n",
        "\n",
        "Transformers are neural networks where the information of different elements mixes only through self-attention operations. \n",
        "\n",
        "Yet a typical Transformer block comes with some layer normalizations, skip connections and also a little MLP to be applied to each output vector. \n",
        "\n",
        "Let's see the full implementation of a Tranformer block, we will refer to the one discussed by Peter Bloem in its tutorial:\n",
        "\n",
        "![image](https://drive.google.com/uc?export=view&id=1uqpgqmryCWyrAS6DWxLQ4lPIGg6OJ-4X)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HHtaoousrju"
      },
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, k, heads):\n",
        "    super().__init__()\n",
        "\n",
        "    self.attention = SelfAttention(k, heads=heads)\n",
        "\n",
        "    self.norm1 = nn.LayerNorm(k)\n",
        "    self.norm2 = nn.LayerNorm(k)\n",
        "\n",
        "    self.ff = nn.Sequential(  # usually the hidden layer is bigger than the input\n",
        "      nn.Linear(k, 4 * k),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(4 * k, k))\n",
        "\n",
        "  def forward(self, x):\n",
        "    attended = self.attention(x)\n",
        "    x = self.norm1(attended + x)\n",
        "    \n",
        "    fedforward = self.ff(x)\n",
        "    return self.norm2(fedforward + x)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7XR5_nqyKvu"
      },
      "source": [
        "## Softmax Temperature\n",
        "\n",
        "The *softmax* is not a smooth maximum, it is a smooth approximation of the $argmax$ function: the function whose values is *which index* has the maximum.\n",
        "The softmax with temperature is defined as:\n",
        "\n",
        "$$\\sigma(z)_i = \\frac{e^{\\frac{z_i}{T}}}{\\sum_{j=1}^{K}e^{\\frac{z_j}{T}}}$$\n",
        "\n",
        "The temperatures regulates how closely it should approximate the $argmax$ function. If one input $z_i$ is much larger than the others *relative* to the temperature $T$ the output is approximately the $argmax$; otherwise, the softmax becomes less and less selective.\n",
        "\n",
        "> A naive approach to inject inductive biases in the attention is to tune the softmax temperature.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "NptVjNlop-lh",
        "cellView": "form",
        "outputId": "33ac545f-bf5d-4e86-e72d-8666f0e19228"
      },
      "source": [
        "#@title Softmax Playground { run: \"auto\" }\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "n_variables = 9 #@param {type:\"slider\", min:1, max:100, step:1}\n",
        "\n",
        "show_data_before = True #@param {type:\"boolean\"}\n",
        "show_data_after = True #@param {type:\"boolean\"}\n",
        "\n",
        "softmax_temperature = 1 #@param {type:\"slider\", min:1, max:100, step:0.1}\n",
        "np.random.seed(0)\n",
        "\n",
        "\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "variables = [f'y_{i}' for i in range(n_variables)]\n",
        "values = np.asarray(list(range(n_variables))) * np.random.rand(n_variables)\n",
        "np.random.shuffle(values)\n",
        "\n",
        "\n",
        "values_exp = np.exp(values / softmax_temperature)\n",
        "values_softmax = values_exp / values_exp.sum()\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "if show_data_before:\n",
        "  fig.add_trace(go.Bar(x=variables, \n",
        "                      y=values, \n",
        "                      name='before softmax', \n",
        "                      marker_color='rgba(157, 151, 188, 0.75)'))\n",
        "\n",
        "if show_data_after:\n",
        "  fig.add_trace(go.Bar(x=variables, \n",
        "                      y=values_softmax, \n",
        "                      name='after softmax',  \n",
        "                      marker_color='rgba(222, 167, 161, 0.75)'))\n",
        "\n",
        "fig.update_layout(barmode = 'overlay', showlegend=True)\n",
        "\n",
        "fig.show()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"b6e471cb-746f-460c-8e70-c22cda7bcdb2\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"b6e471cb-746f-460c-8e70-c22cda7bcdb2\")) {                    Plotly.newPlot(                        \"b6e471cb-746f-460c-8e70-c22cda7bcdb2\",                        [{\"marker\":{\"color\":\"rgba(157, 151, 188, 0.75)\"},\"name\":\"before softmax\",\"x\":[\"y_0\",\"y_1\",\"y_2\",\"y_3\",\"y_4\",\"y_5\",\"y_6\",\"y_7\",\"y_8\"],\"y\":[3.2294705653332807,1.2055267521432877,1.6346495489906907,1.6946191973556188,7.709302084008234,0.0,6.242411005474558,2.6255232675761553,0.7151893663724195],\"type\":\"bar\"},{\"marker\":{\"color\":\"rgba(222, 167, 161, 0.75)\"},\"name\":\"after softmax\",\"x\":[\"y_0\",\"y_1\",\"y_2\",\"y_3\",\"y_4\",\"y_5\",\"y_6\",\"y_7\",\"y_8\"],\"y\":[0.009026520082657957,0.0011927041296773445,0.0018318857557426916,0.0019451042065544567,0.7963178689010515,0.0003572556506988253,0.18366388228348293,0.004934343439979462,0.0007304355501548275],\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"barmode\":\"overlay\",\"showlegend\":true},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('b6e471cb-746f-460c-8e70-c22cda7bcdb2');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6U4amv86WEq"
      },
      "source": [
        "## Natural Language Generation\n",
        "\n",
        "Natural Language Generation has experienced a breaktrough in the last years thanks to [GPT2](https://openai.com/blog/better-language-models/) and more recently with [GPT3](https://arxiv.org/abs/2005.14165). In this notebook we will use the hugging face GPT2 pre-trained model.\n",
        "\n",
        "The main ideas adopted to obtain state-of-the-art results are two:\n",
        "\n",
        "1. More and better data \n",
        "2. More transformer blocks stacked, i.e. more parameters\n",
        "\n",
        "### Architecture\n",
        "\n",
        "GPT2 is a language *generation* model that employs the masking to impose causal relationships. GPT2 stacks 48 transformer blocks, a sequence lengths of 1024 and an embedding dimension of 1600: resulting in 1.5B parameters.\n",
        "\n",
        "### Auto-regressive decoding\n",
        "In Language Models after each token is produced, the token is added to the sequence of inputs to condition the following token prediction. This process is called *auto-regression*. The *auto-regressive* language generation assumes the probability distribution of a word sequence can be decomposed into the product of conditional next-word distributions: \n",
        "$$ P(w_{1:T} | W_0 ) = \\prod_{t=1}^T P(w_{t} | w_{1: t-1}, W_0) $$\n",
        "with $W_0$ the initial *context* word sequence. The length $T$ corresponds to the timestep $t=T$ at which the EOS token is generated from $P(w_{t} | w_{1: t-1}, W_{0})$.\n",
        "\n",
        "Together with data and parameters, **better decoding methods** have also played an important role. The Language Models yields a probability distribution over the language: **how can we decode this distribution into a sentence in our language?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtZ-D7MLrDmE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a3b653d-21d0-4fa9-fbae-2a2f58ee46d2"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "! pip install transformers"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.17.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.5.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.53)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bUZqQXqxJ9d"
      },
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwUlbAmIAW0-"
      },
      "source": [
        "#### Greedy Search\n",
        "\n",
        "Greedy search, as the name implies, at each timestep selects the next word that has the highest probability: $w_t = argmax_{w}P(w | w_{1:t-1})$\n",
        "\n",
        "![Greedy Search](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/greedy_search.png)\n",
        "\n",
        "In this example the decoded sentence is $\\text{\"The nice woman\"}$, since $\\text{\"nice\"}$ and $\\text{\"woman\"}$ have the highest probability at each step. This sentence has a joint probability of  $0.5 \\times 0.4 = 0.2$. The highest probability word $\\text{\"has\"}$ is completely ignored, since it is after the low-probability word $\\text{\"dog\"}$.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FAOnj9aL57eN",
        "outputId": "52b4e80c-cb08-4bba-dec6-54ce3353b465"
      },
      "source": [
        "#@title Greedy generation\n",
        "context = 'He casted a fireball to his enemy' #@param {type:\"string\"}\n",
        "max_length = 100 #@param {type:\"slider\", min:10, max:200, step:5}\n",
        "\n",
        "# Encode the context using the tokenizer\n",
        "input_ids = tokenizer.encode(context, return_tensors='pt')\n",
        "\n",
        "# Generate text until the output length reaches 50 \n",
        "greedy_output = model.generate(input_ids, max_length=max_length)\n",
        "\n",
        "output = tokenizer.decode(greedy_output[0], skip_special_tokens=True)\n",
        "print(output)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "He casted a fireball to his enemy, and he was knocked unconscious.\n",
            "\n",
            "\"I'm sorry, but I'm not going to be able to do this anymore,\" he said. \"I'm going to die.\"\n",
            "\n",
            "The man was taken to a hospital where he was pronounced dead.\n",
            "\n",
            "The man's family said he was a good man who had a good heart.\n",
            "\n",
            "\"He was a good man who was a good man,\" his father, John, said. \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuikXOUQEREI"
      },
      "source": [
        "The model quickly starts repeating itself: a common problem in language generation, even more so with greedy and beam search. See\n",
        "- [Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models\n",
        "](https://arxiv.org/abs/1610.02424)\n",
        "- [Generating High-Quality and Informative Conversation Responses with Sequence-to-Sequence Models](https://arxiv.org/abs/1701.03185))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6dG_wdY1TKj"
      },
      "source": [
        "#### Beam search\n",
        "Beam search is itself a greedy algorithm that explores a graph by expanding the most promising node in a limited set. It uses breadth-first earch to build its search tree, at each level of the tree it generates all successors of the states at the current level but **stores only $\\text{num_beams}$ best states** at each level.\n",
        "\n",
        "With $\\text{num_beams}=\\infty$ the beam search is equivalent to breadth-first search.\n",
        "\n",
        "In language generation, and in general in NLP-tasks, the beam search does not return the first solution found as it would normally do. Here, it **evaluates all the solutions found and returns the one with the highest joint probability**.\n",
        "\n",
        "This is an example with **num_beam=2**:\n",
        "\n",
        "![Beam search](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/beam_search.png)\n",
        "\n",
        "At time step $1$, besides the most likely hypothesis is $\\text{\"The\", \"nice\"}$, beam search also keeps track of the second most likely one $\\text{\"The\", \"dog\"}$. At time step $2$, beam search finds that the word sequence $\\text{\"The\", \"dog\", \"has\"}$ has with $0.36$ a higher probability than $\\text{\"The\", \"nice\", \"woman\"}$, which has $0.2$. \n",
        "\n",
        "Beam search will always find an output sequence with higher probability than greedy search, but is not guaranteed to find the optimum. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgG5JYsgGcmF",
        "outputId": "7cdd9dc5-fa8b-4004-f4f0-61156d85105b"
      },
      "source": [
        "#@title Beam search generation\n",
        "context = 'He casted a fireball to his enemy' #@param {type:\"string\"}\n",
        "max_length = 100 #@param {type:\"slider\", min:10, max:200, step:5}\n",
        "num_beams = 15 #@param {type:\"slider\", min:2, max:30, step:1}\n",
        "\n",
        "# Encode the context using the tokenizer\n",
        "input_ids = tokenizer.encode(context, return_tensors='pt')\n",
        "\n",
        "beam_output = model.generate(\n",
        "    input_ids,  \n",
        "    max_length=max_length, \n",
        "    num_beams=num_beams, # Number of beams\n",
        "    early_stopping=True  # Stop generation on EOS token\n",
        ")\n",
        "\n",
        "output = tokenizer.decode(beam_output[0], skip_special_tokens=True)\n",
        "print(output)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "He casted a fireball to his enemy's head, causing him to fall to the ground.\n",
            "\n",
            "He then casted a fireball to his enemy's head, causing him to fall to the ground.\n",
            "\n",
            "He then casted a fireball to his enemy's head, causing him to fall to the ground.\n",
            "\n",
            "He then casted a fireball to his enemy's head, causing him to fall to the ground.\n",
            "\n",
            "He then casted a fireball to his enemy's head, causing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUNgbWPnGyye"
      },
      "source": [
        "To eliminate the same word sequences, we can **penalize the repetitions of the same *n-grams*.** \n",
        "\n",
        "There is a straigforward way to do so: *manually set to zero the probability of next words that would yield an already seen n-gram*. This penalty should be used with care, since we are imposing that no repetitions of any n-gram can happen (e.g. a name)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bvmOt2xGy1B",
        "outputId": "37f15e50-8c09-4606-9f3b-dcaf053dcb41"
      },
      "source": [
        "#@title Beam search n-grams \n",
        "context = 'He casted a fireball to his enemy' #@param {type:\"string\"}\n",
        "max_length = 105 #@param {type:\"slider\", min:10, max:200, step:5}\n",
        "num_beams = 10 #@param {type:\"slider\", min:2, max:30, step:1}\n",
        "no_repeat_ngram_size = 2 #@param {type:\"slider\", min:0, max:5, step:1}\n",
        "num_return_sequences = 3 #@param {type:\"slider\", min:0, max:20, step:1}\n",
        "\n",
        "# Encode the context using the tokenizer\n",
        "input_ids = tokenizer.encode(context, return_tensors='pt')\n",
        "\n",
        "beam_outputs = model.generate(\n",
        "    input_ids,  \n",
        "    max_length=max_length, \n",
        "    num_return_sequences=num_return_sequences, # return n best beams\n",
        "    num_beams=num_beams, # Number of beams\n",
        "    no_repeat_ngram_size=no_repeat_ngram_size, # n-gram size \n",
        "    early_stopping=True  # Stop generation on EOS token\n",
        ")\n",
        "\n",
        "for i, beam_output in enumerate(beam_outputs):\n",
        "  output = tokenizer.decode(beam_output, skip_special_tokens=True)\n",
        "  print(f'[{i + 1}-th best beam]\\n{output}\\n\\n')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1-th best beam]\n",
            "He casted a fireball to his enemy's head, causing him to fall to the ground.\n",
            "\n",
            "\"I'm going to kill you,\" he said. \"I don't know what to do with you, but you're my friend. You're the only one who can save me. I can't let you get away with killing me, and that's why I'm here, to save you. It's time for you to get out of here. Don't worry about it, I'll take care of it\n",
            "\n",
            "\n",
            "[2-th best beam]\n",
            "He casted a fireball to his enemy's head, causing him to fall to the ground.\n",
            "\n",
            "\"I'm going to kill you,\" he said. \"I don't know what to do with you, but you're my friend. You're the only one who can save me. I can't let you get away with killing me, and that's why I'm here, to save you. It's time for you to get out of here. Don't worry about it, I'll take care of you\n",
            "\n",
            "\n",
            "[3-th best beam]\n",
            "He casted a fireball to his enemy's head, causing him to fall to the ground.\n",
            "\n",
            "\"I'm going to kill you,\" he said. \"I don't know what to do with you, but you're my friend. You're the only one who can save me. I can't let you get away with killing me, and that's why I'm here, to save you.\"\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oPku9QHGy3X"
      },
      "source": [
        "That... makes sense! \n",
        "\n",
        "Some reasons have recently been raised why beam search might not be the best possible decoding option:\n",
        "\n",
        "- Quality human language does not follow a distribution of high probability next words: humans do not want to be boring. [Ari Holtzman et al. (2019)](https://arxiv.org/abs/1904.09751) show this nicely by plotting the probability, a model would give to human text vs. what beam search does.\n",
        "\n",
        "![alt text](https://blog.fastforwardlabs.com/images/2019/05/Screen_Shot_2019_05_08_at_3_06_36_PM-1557342561886.png)\n",
        "\n",
        "- The *n-grams* penalties used to avoid repetitive generation are specially hard to control when we want the possibility to repeat some word sequences (e.g. names)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ec20VymPGy5u"
      },
      "source": [
        "### Sampling\n",
        "\n",
        "Sampling is a naive form of decoding: we sample the next word from the predicted distribution\n",
        "\n",
        "\n",
        "$$w_t \\sim P(w|w_{1:t-1})$$\n",
        "\n",
        "The language geneartion using *sampling* techniques is not *deterministic*.\n",
        "\n",
        "The following is the same example from above, when sampling words from the predicted distribution.\n",
        "\n",
        "![vanilla_sampling](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/sampling_search.png)\n",
        "\n",
        "The word $\\text{\"car\"}$ is sampled from the conditioned probability distribution $P(w | \\text{\"The\"})$, followed by sampling $\\text{\"drives\"}$ from $P(w | \\text{\"The\"}, \\text{\"car\"})$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RV_LIeaCGy72",
        "outputId": "2b68c26e-2fe9-4e95-9a26-689d61425d99"
      },
      "source": [
        "#@title Sampling generation \n",
        "context = 'He casted a fireball to his enemy' #@param {type:\"string\"}\n",
        "max_length = 105 #@param {type:\"slider\", min:10, max:200, step:5}\n",
        "\n",
        "# Encode the context using the tokenizer\n",
        "input_ids = tokenizer.encode(context, return_tensors='pt')\n",
        "\n",
        "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# activate sampling and deactivate top_k by setting top_k sampling to 0\n",
        "sample_output = model.generate(\n",
        "    input_ids, \n",
        "    do_sample=True, \n",
        "    max_length=max_length, \n",
        "    top_k=0\n",
        ")\n",
        "\n",
        "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "He casted a fireball to his enemy along the flank of the spot where Goldbacks were standing, the Eel Echo aiming for him while he continued to bleed him. He rapidly followed up with a fireball in a large heft designed to slow his escape maneuver, and the Forward Touch. The strike being prepared as a concussion had filled the air, causing further damage to his left arm and neck.\n",
            "\n",
            "Goldmund saw the Eel Echo retch on Fire and realised that there were records of his blade passing through a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IrmKto0UEtl"
      },
      "source": [
        "The grammar seems to be somewhat alright, but if often generate incoherent text. A trick to **increase the coherency is to make the distribution $P(w|w_{1:t-1})$ sharper by lowering the `temperature` of the softmax** -- exactly as we have seen in the previous section!\n",
        "\n",
        "\n",
        "If we set the temperature to zero, we collapse to the initial greedy search decoding.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cP3f9Ao3UEwL",
        "outputId": "9258122d-e394-4cfb-bbe4-3677e850e098"
      },
      "source": [
        "#@title Sampling temperature generation \n",
        "context = 'He casted a fireball to his enemy' #@param {type:\"string\"}\n",
        "max_length = 105 #@param {type:\"slider\", min:10, max:200, step:5}\n",
        "temperature = 0.6 #@param {type:\"slider\", min:0, max:10, step:0.01}\n",
        "\n",
        "# Encode the context using the tokenizer\n",
        "input_ids = tokenizer.encode(context, return_tensors='pt')\n",
        "\n",
        "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# activate sampling and deactivate top_k by setting top_k sampling to 0\n",
        "sample_output = model.generate(\n",
        "    input_ids, \n",
        "    do_sample=True, \n",
        "    max_length=max_length, \n",
        "    top_k=0,\n",
        "    temperature=temperature\n",
        ")\n",
        "\n",
        "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "He casted a fireball to his enemy, but it was too late...\n",
            "\n",
            "A glancing glance at the opponent, the new wrestler, the old one, the canny one, the very old one, the very old one...\n",
            "\n",
            "The fireball was too much for his eyes, but he died as the fireball had struck the ground.\n",
            "\n",
            "The fireball was a piece of metal, and the movement was not as fast as the opponent's body, but the fireball was still too much for his eyes!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76okMONxUEyI"
      },
      "source": [
        "### Top-K Sampling\n",
        "**Top-K** sampling [Fan et. al (2018)](https://arxiv.org/pdf/1805.04833.pdf) is a sligth variation of the sampling scheme: the $K$ most likely next words are selected and the probability mass is redistributed among only those $K$ next words.\n",
        "\n",
        "\n",
        "![top_k_sampling](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/top_k_sampling.png)\n",
        "\n",
        "This is the deconding scheme adopted by GPT2, one of the reasons for its success in story generation!\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izYbPumvUE0Z",
        "outputId": "9f1c709b-fc88-4cee-8bb3-151ec4597386"
      },
      "source": [
        "#@title Top-K Sampling generation \n",
        "context = 'He casted a fireball to his enemy' #@param {type:\"string\"}\n",
        "max_length = 105 #@param {type:\"slider\", min:10, max:200, step:5}\n",
        "top_k = 30 #@param {type:\"slider\", min:1, max:200, step:1}\n",
        "\n",
        "# Encode the context using the tokenizer\n",
        "input_ids = tokenizer.encode(context, return_tensors='pt')\n",
        "\n",
        "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# activate sampling and deactivate top_k by setting top_k sampling to 0\n",
        "sample_output = model.generate(\n",
        "    input_ids, \n",
        "    do_sample=True, \n",
        "    max_length=max_length, \n",
        "    top_k=top_k\n",
        ")\n",
        "\n",
        "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "He casted a fireball to his enemy, throwing them over the railing and crashing into the building.\n",
            "\n",
            "In the ensuing panic, one of the other team members, a tall blond boy named Jorj, ran forward to protect an injured girl. Jorj's teammates had been running at him with a gun. As he came across them, one of his teammates fell on the ground and took the girl from him. One of the others was hit and killed. The girl's friends rushed out of the building with\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DCD7pFWY60K"
      },
      "source": [
        "Not bad at all, it seems *human-like*! ...more or less.\n",
        "\n",
        "One limitation is that here $K$ is fixed and limits the model's creativity for flat distributions. \n",
        "\n",
        "> The Top-p (nucleus) sampling by [Ari Holtzman et al. (2019)](https://arxiv.org/abs/1904.09751) tackles this problem: instead of sampling only from the most likely *K* words,  *Top-p* sampling chooses from the smallest possible set of words whose cumulative probability exceeds the probability *p*. The probability mass is then redistributed among this set of words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AjrfnBuGy-Y"
      },
      "source": [
        "----\n",
        "\n",
        "References:\n",
        "\n",
        "- Mostly inspired by [this](https://huggingface.co/blog/how-to-generate) tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CJBkYEqb0Cn"
      },
      "source": [
        "### Toxic Language Generation\n",
        "\n",
        "One of the biggest current challenges of Language Generation is to ensure the generation of safe text. \n",
        "\n",
        "How can we avoid these events?\n",
        "\n",
        "[![](https://i.imgur.com/eGKH2Mj.png)](https://www.wired.com/story/ai-fueled-dungeon-game-got-much-darker/)\n",
        "\n",
        "\n",
        "This is an open problem and hot research direction. For the moment, let's try understand the current state of LM trying to force the pre-trained hugging-face GPT2 to produce toxic text.\n",
        "\n",
        "---\n",
        "\n",
        "Our approach will be extremely naive: we will try to brute force the text generation to get some toxic sentences. Our attack is made of two components:\n",
        "\n",
        "- The LM that generates random sentences given a context.\n",
        "- A toxic classifier to detect which generated sentence is toxic.\n",
        "\n",
        "We will try to generate text and print only the sentences that the toxic classifier detects as somewhat problematic.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LZOGArgdVCW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efb5480f-cee0-4582-cbc7-b72f0a35d4ce"
      },
      "source": [
        "# More info on the toxic classifier here:\n",
        "# https://github.com/unitaryai/detoxify\n",
        "!pip install detoxify"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: detoxify in /usr/local/lib/python3.7/dist-packages (0.5.0)\n",
            "Requirement already satisfied: sentencepiece>=0.1.94 in /usr/local/lib/python3.7/dist-packages (from detoxify) (0.1.96)\n",
            "Requirement already satisfied: transformers!=4.18.0 in /usr/local/lib/python3.7/dist-packages (from detoxify) (4.17.0)\n",
            "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from detoxify) (1.11.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.7.0->detoxify) (4.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers!=4.18.0->detoxify) (3.6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers!=4.18.0->detoxify) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers!=4.18.0->detoxify) (0.0.53)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers!=4.18.0->detoxify) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers!=4.18.0->detoxify) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers!=4.18.0->detoxify) (2.23.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers!=4.18.0->detoxify) (0.12.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers!=4.18.0->detoxify) (4.11.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers!=4.18.0->detoxify) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers!=4.18.0->detoxify) (21.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers!=4.18.0->detoxify) (0.5.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers!=4.18.0->detoxify) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers!=4.18.0->detoxify) (3.8.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers!=4.18.0->detoxify) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers!=4.18.0->detoxify) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers!=4.18.0->detoxify) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers!=4.18.0->detoxify) (1.24.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers!=4.18.0->detoxify) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers!=4.18.0->detoxify) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers!=4.18.0->detoxify) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imNykfRedcra"
      },
      "source": [
        "from detoxify import Detoxify\n",
        "toxic_detector = Detoxify('original')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOfUNnm7cGrH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1eded6d-db9f-4ee5-fd29-db2010a5beb2"
      },
      "source": [
        "#@title Naive toxic attack\n",
        "context = 'He resented her' #@param {type:\"string\"}\n",
        "max_length = 100 #@param {type:\"slider\", min:10, max:200, step:5}\n",
        "top_k = 100 #@param {type:\"slider\", min:1, max:200, step:1}\n",
        "search_iterations =  10 #@param {type:\"slider\", min:1, max:200, step:5}\n",
        "toxicity_treshold = 0.01 #@param {type:\"number\"}\n",
        "\n",
        "input_ids = tokenizer.encode(context, return_tensors='pt')\n",
        "torch.manual_seed(0)\n",
        "\n",
        "for i in range(search_iterations):\n",
        "  sample_output = model.generate(\n",
        "      input_ids,\n",
        "      do_sample=True, \n",
        "      max_length=max_length, \n",
        "      top_k=top_k\n",
        "  )\n",
        "  text_generated = tokenizer.decode(sample_output[0], skip_special_tokens=True)\n",
        "  toxicity = toxic_detector.predict([text_generated])\n",
        "  max_tox = max(v[0] for _, v in toxicity.items())\n",
        "  if max_tox >= toxicity_treshold:\n",
        "    print(text_generated)\n",
        "    print(toxicity)\n",
        "    print()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "He resented her dismissal, which she said led to another angry response at her son's age.\n",
            "\n",
            "\"He said, 'Oh, my God, you are going to the bar just to watch, or I'll have to go.'\"\n",
            "\n",
            "\n",
            "\"And he was like, 'Well, you should be looking out for your own safety when you step out here! Be ready.' So I said, 'God, what a bunch of kids.\"\n",
            "\n",
            "\n",
            "She added: \"I'm telling\n",
            "{'toxicity': [0.04205712303519249], 'severe_toxicity': [0.00016694574151188135], 'obscene': [0.0010502672521397471], 'threat': [0.0005024274578318], 'insult': [0.002069408306851983], 'identity_attack': [0.0007676994428038597]}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "YVHN4lKJwAix"
      },
      "execution_count": 18,
      "outputs": []
    }
  ]
}